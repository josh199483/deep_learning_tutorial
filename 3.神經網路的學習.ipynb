{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以辨識手寫數字為例\n",
    "# 1.以人類觀察的方式來找出規則性，舉例來說如果人類要知道這個圖像是0，可能可以以經驗來推斷1是由上往下的一直線，來判定該數字是否為1\n",
    "# 2.機器學習，人類想出來的特徵量(輸入資料裡擷取的重要資料，影像辨識領域中有一些有名的特徵量，例:SIFT、SURF等等)，並且用機器學習演算法來做辨識\n",
    "# 3.深度學習，電腦可以從圖像本身自己擷取特徵量，並自行分類數字的類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在機器學習問題中，通常會分成訓練資料和測試資料\n",
    "# 先用訓練資料進行學習，找到最佳模型參數，接著在用測試資料評估訓練後模型的performance\n",
    "# 因為這個模型最重要的是他的一般化能力\n",
    "# 也就是評估未知資料的能力!!\n",
    "# 一個模型如果缺乏一般化的能力，就會變成過度擬合(也就是overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在神經網路裡，會使用損失函數(loss function)來當作指標，從中尋找模型的最佳參數\n",
    "# 目前比較常使用的損失函數有兩個，1.均方誤差(mean squared error) 2.交叉商誤差(cross entropy error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.均方誤差(mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.6437499999999999\n"
     ]
    }
   ],
   "source": [
    "# 公式是 E = 1/2 * sigma(k)(yk - tk)^2\n",
    "# 有點難用文字表達...，可以自行google\n",
    "# 其中 yk 代表神經網路輸出的結果， tk 代表訓練資料，k 是資料的維度\n",
    "# 以手寫辨識數字為例，y = [0.1,0.6,0.05,0.0,...] ，t = [0,1,0,0,....]\n",
    "# 神經網路輸出的 y 是 softmax 函數的輸出，可以看為機率!!代表1的機率是0.6\n",
    "# t的表達方式是正確答案標籤用1來表達，其餘錯誤標籤都是0，範例是index為1的時候數字是1，所以正確標籤是1，這種表達方式稱為one-hot!!\n",
    "# 所以這個公式就是計算 \"神經網路輸出結果\" 和 \"正確標籤\" 的各元素差的總和\n",
    "# 也因此若神經網路輸出結果越接近正確答案時，MSE會越小，直到趨近於0\n",
    "# 當 t = [0,1,0,0,0,0,0,0,0,0]，而 y = [0,1,0,0,0,0,0,0,0,0]時，MSE會等於0，也就是輸出結果最適合訓練資料\n",
    "\n",
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "# 假設辨識手寫數字的正確答案是1\n",
    "t = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 1的機率最高時\n",
    "y = [0.1, 0.6, 0.05, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "# 6的機率最高時，損失函數的值比較大\n",
    "y = [0.1, 0.05, 0.05, 0.0, 0.05, 0.6, 0.0, 0.1, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.交叉商誤差(cross entropy error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230e4c565c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.9957302735559908\n"
     ]
    }
   ],
   "source": [
    "# 公式是 E = -sigma(k)(tk * log(以e為底的對數)(yk))\n",
    "# 有點難用文字表達...，可以自行google\n",
    "# 數字越小越準確，但實際上只有tk = 1才會有值，其餘皆為0\n",
    "# 因為tk是正確答案的label，是以one-hot來表示，正確答案為1，其餘為0\n",
    "# 例如:假設1是正確答案，並且對應的神經網路輸出是0.5，交叉商誤差就是 -ln(0.6) = 0.51，ln是以e為底數的自然對數的表示方法\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.01, 1.0, 0.001)\n",
    "y = np.log(x)\n",
    "# 把階梯函數畫出來，是個以0為界線，輸出只有0和1而已\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "# 這個圖是y = ln(x)的圖表\n",
    "# 可以看到x = 1的時候，y = 0，隨著x趨近於0，y值將會變得趨近負無限大\n",
    "# 也就是說當np.log(0)的時候，會變成負無限大造成無法運算\n",
    "# 所以cross_entropy_error方法要加入微小值(delta)，避免發生無限大的狀況\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 0.0000001\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# 假設辨識手寫數字的正確答案是1\n",
    "t = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 1的機率最高時\n",
    "y = [0.1, 0.6, 0.05, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "# 6的機率最高時，損失函數的值比較大\n",
    "y = [0.1, 0.05, 0.05, 0.0, 0.05, 0.6, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.小批次學習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44514, 47199, 24979, 23825, 49778, 26025, 54961, 29350,  8177,\n",
       "       53702])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假設訓練資料筆數有100筆，那100個損失函數的和就是一個指標，只是為了不讓資料筆數影響損失函數的指標，會把總和再除以100筆，得到平均損失函數!\n",
    "# 以辨識手寫數字的MNIST資料集來說，有60000筆資料，如果要計算60000筆損失函數的平均要花費一些時間\n",
    "# 但如果是大數據，資料量達到千萬，甚至億以上的時候，就不太可能計算平均損失函數了\n",
    "# 所以在訓練神經網路時，會從訓練資料隨機挑選一部份資料，稱做小批次，例如60000筆資料挑選100筆資料做學習，就稱做小批次學習\n",
    "\n",
    "# 這邊先來個小批次學習的練習\n",
    "# 可試著自己載入dataset，不管是pandas或tensorflow整理的資料集，以下方式不能在這裡執行\n",
    "# import numpy as np\n",
    "# from dataset.mnist import load_mnist\n",
    "# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "# print(x_train.shape) # (60000, 784)\n",
    "# print(t_train.shape) # (60000, 10)\n",
    "\n",
    "# 如果要從60000筆資料隨機挑出10筆\n",
    "# 下面是大致概念\n",
    "# train_size = x_train.shape[0]\n",
    "# batch_size = 10\n",
    "# batch_mask = np.random.choice(train_size, batch_size) # 取得隨機index的陣列\n",
    "# x_batch = x_train[batch_mask]\n",
    "# t_batch = t_train[batch_mask]\n",
    "np.random.choice(60000, 10) # 會得到隨機挑選的10個index的陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以批次版來執行cross entropy error\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size\n",
    "\n",
    "# 如果不是使用one-hot，而是2或是6這種標籤，function可改為這樣\n",
    "# 這邊還需要在更詳細的補充~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.數值微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 因為接下來要介紹很重要的梯度觀念，所以先從微分開始講起\n",
    "# 微分代表某個瞬間的變化量\n",
    "# 例如:某個時間前1分鐘的跑步距離，縮短到某個時間前0.1秒的跑步距離，慢慢縮小就可以得到某瞬間的變化量(跑步距離)\n",
    "# https://zh.wikipedia.org/wiki/%E6%95%B8%E5%80%BC%E5%BE%AE%E5%88%86，維基百科的有限差分法就是在說這個，可參考\n",
    "# 通常看到df(x)/dx，所代表的就是f(x)這個函數對x作微分(也就是相對於x，f(x)的變化)!!\n",
    "# 下面是數值微分的實現，只需在h代入極小值即可\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "# 但上面的方法會有兩個問題\n",
    "# 1.雖然h使用了極小的值，但這樣反而會造成捨去誤差(rounding error)，也就是小數的一部分範圍會被捨去，以python為例\n",
    "print(np.float32(1e-50)) # 會得到0.0，使用太小的值，電腦在運算時會出現問題，這邊會把h改為1e-4\n",
    "# 2.上面函數計算了x+h和x的差分，但這個方法本身就會造成誤差\n",
    "# \"真微分\"是對應x位置的斜率(切線)，但上面函數計算的卻是x+h和x的斜率，這個差異是因為h無法無限趨近於0所造成!!\n",
    "\n",
    "# 因此計算x+h和x-h的差分可以減少誤差，這種差分稱為中央差分(x+h和x的差分稱為前差分)，改善兩個問題後的函數如下\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyISJBT4IiJB4v8Avs8d1Tqeb80AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230e4f94400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# 來看個範例 y = 0.01 * x^2 + 0.1 * x\n",
    "def func1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "# 來看對應x=5和x=10的微分值，用數值微分的方式來計算\n",
    "print(numerical_diff(func1, 5)) # 趨近於0.2\n",
    "print(numerical_diff(func1, 10)) # 趨近於0.3\n",
    "\n",
    "# 如果用解析解的方式來計算微分值\n",
    "# 解析解的意思就是先對函數做微分，以上例來看y = 0.01 * x^2 + 0.1 * x，會變成dy/dx = 0.02*x + 0.1\n",
    "# 這時候代入x=5和x=10會分別得到0.2和0.3的值，與用數值微分的方式計算的值幾乎等同，因誤差非常小，所以用程式計算時都用數值微分來計算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230e4dff8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 畫出切線\n",
    "# 還不是很清楚式怎麼畫的...\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y # 回傳一個function，t就是該function會傳進來的參數\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(func1, 5)\n",
    "y2 = tf(x) # 這個x就是上面提到function傳進來的參數\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.偏微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "# f(x0, x1) = x0^2 + x1^2\n",
    "# 用程式來實現\n",
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # return np.sum(x**2) # 可應付更多變數的狀況\n",
    "    \n",
    "# 要對這個函數進行微分時，就會考慮到要對哪個變數做微分，因此由多個變數組成的函數微分就是偏微分\n",
    "# 偏微分是在多個變數中，鎖定某個變數，將其他變數固定成某數值\n",
    "# 當x0 = 3，x1 = 4時，計算x0的偏微分\n",
    "def func_par1(x0):\n",
    "    return x0**2 + 4.0**2 \n",
    "print(numerical_diff(func_par1, 3.0))\n",
    "\n",
    "# 當x0 = 3，x1 = 4時，計算x1的偏微分\n",
    "def func_par2(x1):\n",
    "    return 3.0**2 + x1**2 \n",
    "print(numerical_diff(func_par2, 4.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.梯度!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]]\n",
      "[0 0] \n",
      "\n",
      "[6. 8.] \n",
      "\n",
      "[0. 4.]\n"
     ]
    }
   ],
   "source": [
    "# 要統一進行(x0, x1)的偏微分時，等於是計算(df/dx0, df/dx1)，這邊因為沒辦法打出偏微分的表達符號，所以還是用d來表示\n",
    "# 這種把函數裡全部變數的偏微分當作向量的方式就稱為 \"梯度\"!!!\n",
    "print(np.zeros_like(np.array([[0],[2]])))\n",
    "print(np.zeros_like(np.array([0,2])),'\\n')\n",
    "# 來用程式定義一下吧~\n",
    "def num_gra(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # 產生和x相同形狀的陣列，element都是0\n",
    "    # grad = np.zeros(x.shape)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 回復原值\n",
    "        \n",
    "    return grad\n",
    "# 因為輸入的X陣列有可能不是一維的\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return num_gra(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = num_gra(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "print(numerical_gradient(func2,np.array([3.0, 4.0])),'\\n')\n",
    "print(numerical_gradient(func2,np.array([0.0, 2.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 梯度的向量基本上指向最低的位置，但實際上卻不一定如此\n",
    "# 比較準確的說法是，梯度顯示的方向是，函數值減少最多的方向!!!\n",
    "# 在學習神經網路時，就是要找到最適合的參數，也就是損失函數值最小的參數\n",
    "# 但損失函數一般很複雜而且參數空間太大，因此善用梯度，盡量找出損失函數的最小值，就是所謂的梯度法\n",
    "\n",
    "# 因為前面有提到的，梯度是在各個位置，指出函數值減少最多的方向，所以不一定指出的位置就是最小值，甚至方向也不一定對!!\n",
    "# 函數的極小值、最小值、鞍點(saddle point)，梯度都為0，所以也有可能用梯度法找到的結果是除了最小值以外的結果\n",
    "# 因為梯度法是會隨著要找出最小值獲最大值而變化，若要找出最小值時，稱作梯度下降法(gradient descent method)，\n",
    "# 反之就是梯度上升法，但其實只差在正負號的反轉，所以本質上是沒有差別的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-6.11110793e-10,  8.14814391e-10]), array([[-3.00000000e+00,  4.00000000e+00],\n",
      "       [-2.40000000e+00,  3.20000000e+00],\n",
      "       [-1.92000000e+00,  2.56000000e+00],\n",
      "       [-1.53600000e+00,  2.04800000e+00],\n",
      "       [-1.22880000e+00,  1.63840000e+00],\n",
      "       [-9.83040000e-01,  1.31072000e+00],\n",
      "       [-7.86432000e-01,  1.04857600e+00],\n",
      "       [-6.29145600e-01,  8.38860800e-01],\n",
      "       [-5.03316480e-01,  6.71088640e-01],\n",
      "       [-4.02653184e-01,  5.36870912e-01],\n",
      "       [-3.22122547e-01,  4.29496730e-01],\n",
      "       [-2.57698038e-01,  3.43597384e-01],\n",
      "       [-2.06158430e-01,  2.74877907e-01],\n",
      "       [-1.64926744e-01,  2.19902326e-01],\n",
      "       [-1.31941395e-01,  1.75921860e-01],\n",
      "       [-1.05553116e-01,  1.40737488e-01],\n",
      "       [-8.44424930e-02,  1.12589991e-01],\n",
      "       [-6.75539944e-02,  9.00719925e-02],\n",
      "       [-5.40431955e-02,  7.20575940e-02],\n",
      "       [-4.32345564e-02,  5.76460752e-02],\n",
      "       [-3.45876451e-02,  4.61168602e-02],\n",
      "       [-2.76701161e-02,  3.68934881e-02],\n",
      "       [-2.21360929e-02,  2.95147905e-02],\n",
      "       [-1.77088743e-02,  2.36118324e-02],\n",
      "       [-1.41670994e-02,  1.88894659e-02],\n",
      "       [-1.13336796e-02,  1.51115727e-02],\n",
      "       [-9.06694365e-03,  1.20892582e-02],\n",
      "       [-7.25355492e-03,  9.67140656e-03],\n",
      "       [-5.80284393e-03,  7.73712525e-03],\n",
      "       [-4.64227515e-03,  6.18970020e-03],\n",
      "       [-3.71382012e-03,  4.95176016e-03],\n",
      "       [-2.97105609e-03,  3.96140813e-03],\n",
      "       [-2.37684488e-03,  3.16912650e-03],\n",
      "       [-1.90147590e-03,  2.53530120e-03],\n",
      "       [-1.52118072e-03,  2.02824096e-03],\n",
      "       [-1.21694458e-03,  1.62259277e-03],\n",
      "       [-9.73555661e-04,  1.29807421e-03],\n",
      "       [-7.78844529e-04,  1.03845937e-03],\n",
      "       [-6.23075623e-04,  8.30767497e-04],\n",
      "       [-4.98460498e-04,  6.64613998e-04],\n",
      "       [-3.98768399e-04,  5.31691198e-04],\n",
      "       [-3.19014719e-04,  4.25352959e-04],\n",
      "       [-2.55211775e-04,  3.40282367e-04],\n",
      "       [-2.04169420e-04,  2.72225894e-04],\n",
      "       [-1.63335536e-04,  2.17780715e-04],\n",
      "       [-1.30668429e-04,  1.74224572e-04],\n",
      "       [-1.04534743e-04,  1.39379657e-04],\n",
      "       [-8.36277945e-05,  1.11503726e-04],\n",
      "       [-6.69022356e-05,  8.92029808e-05],\n",
      "       [-5.35217885e-05,  7.13623846e-05],\n",
      "       [-4.28174308e-05,  5.70899077e-05],\n",
      "       [-3.42539446e-05,  4.56719262e-05],\n",
      "       [-2.74031557e-05,  3.65375409e-05],\n",
      "       [-2.19225246e-05,  2.92300327e-05],\n",
      "       [-1.75380196e-05,  2.33840262e-05],\n",
      "       [-1.40304157e-05,  1.87072210e-05],\n",
      "       [-1.12243326e-05,  1.49657768e-05],\n",
      "       [-8.97946606e-06,  1.19726214e-05],\n",
      "       [-7.18357285e-06,  9.57809713e-06],\n",
      "       [-5.74685828e-06,  7.66247770e-06],\n",
      "       [-4.59748662e-06,  6.12998216e-06],\n",
      "       [-3.67798930e-06,  4.90398573e-06],\n",
      "       [-2.94239144e-06,  3.92318858e-06],\n",
      "       [-2.35391315e-06,  3.13855087e-06],\n",
      "       [-1.88313052e-06,  2.51084069e-06],\n",
      "       [-1.50650442e-06,  2.00867256e-06],\n",
      "       [-1.20520353e-06,  1.60693804e-06],\n",
      "       [-9.64162827e-07,  1.28555044e-06],\n",
      "       [-7.71330261e-07,  1.02844035e-06],\n",
      "       [-6.17064209e-07,  8.22752279e-07],\n",
      "       [-4.93651367e-07,  6.58201823e-07],\n",
      "       [-3.94921094e-07,  5.26561458e-07],\n",
      "       [-3.15936875e-07,  4.21249167e-07],\n",
      "       [-2.52749500e-07,  3.36999333e-07],\n",
      "       [-2.02199600e-07,  2.69599467e-07],\n",
      "       [-1.61759680e-07,  2.15679573e-07],\n",
      "       [-1.29407744e-07,  1.72543659e-07],\n",
      "       [-1.03526195e-07,  1.38034927e-07],\n",
      "       [-8.28209562e-08,  1.10427942e-07],\n",
      "       [-6.62567649e-08,  8.83423532e-08],\n",
      "       [-5.30054119e-08,  7.06738826e-08],\n",
      "       [-4.24043296e-08,  5.65391061e-08],\n",
      "       [-3.39234636e-08,  4.52312849e-08],\n",
      "       [-2.71387709e-08,  3.61850279e-08],\n",
      "       [-2.17110167e-08,  2.89480223e-08],\n",
      "       [-1.73688134e-08,  2.31584178e-08],\n",
      "       [-1.38950507e-08,  1.85267343e-08],\n",
      "       [-1.11160406e-08,  1.48213874e-08],\n",
      "       [-8.89283245e-09,  1.18571099e-08],\n",
      "       [-7.11426596e-09,  9.48568795e-09],\n",
      "       [-5.69141277e-09,  7.58855036e-09],\n",
      "       [-4.55313022e-09,  6.07084029e-09],\n",
      "       [-3.64250417e-09,  4.85667223e-09],\n",
      "       [-2.91400334e-09,  3.88533778e-09],\n",
      "       [-2.33120267e-09,  3.10827023e-09],\n",
      "       [-1.86496214e-09,  2.48661618e-09],\n",
      "       [-1.49196971e-09,  1.98929295e-09],\n",
      "       [-1.19357577e-09,  1.59143436e-09],\n",
      "       [-9.54860614e-10,  1.27314749e-09],\n",
      "       [-7.63888491e-10,  1.01851799e-09]]))\n"
     ]
    }
   ],
   "source": [
    "# 用算式來表達梯度下降法，其中lr代表學習率，學習率會決定每一次學習時要更新多少參數，一開始必須先決定一個數值0.01或0.0001\n",
    "# 學習率太大或太小都不好，太大的話會使誤差超過太多，導致無法收斂，太小的話，收斂速度太慢!!\n",
    "# x0 = x0 - lr * (df/dx0)\n",
    "# x1 = x1 - lr * (df/dx1)\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad # x = x - lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(func2, init_x=init_x, lr=0.1)) # array([-6.11110793e-10,  8.14814391e-10]) 趨近於(0,0)\n",
    "# 可以自行試試看把learning rate調很大或很小試試看會發生甚麼事情\n",
    "\n",
    "# 像學習率這種參數稱為超參數(hyperparameter)，與神經網路的weight或bias不一樣，這些是在學習中自動產生的參數，\n",
    "# 超參數則是由人工設定，經過多次試驗找出最適合的參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.神經網路的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始權重:  [[ 1.29820283 -1.14351756 -0.2620714 ]\n",
      " [-1.41530177 -0.25038932 -0.40462839]]\n",
      "預測結果:  [-0.4948499  -0.91146092 -0.52140839]\n",
      "最大值的index:  0\n",
      "損失函數值:  0.9947080794793444\n",
      "梯度:  [[ 0.22787097  0.15023016 -0.37810112]\n",
      " [ 0.34180645  0.22534524 -0.56715169]]\n"
     ]
    }
   ],
   "source": [
    "# 這邊要以一個最簡單的一層神經網路為例，沒有任何的隱藏層(hidden layer)\n",
    "# 所以也會用到第二章提到的activation function和矩陣運算等等概念\n",
    "\n",
    "# 第二章提到的softmax方法，輸出層使用的活化函數，用在分類問題上\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "# 前面提到的損失函數cross_entropy_error\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 0.0000001\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 以常態分佈初始化權重\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "net = simpleNet()\n",
    "print('初始權重: ',net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print('預測結果: ',p)\n",
    "print('最大值的index: ',np.argmax(p))\n",
    "t = np.array([0, 0, 1])\n",
    "print('損失函數值: ',net.loss(x, t))\n",
    "\n",
    "# 接著要計算梯度，這裡用了個無用的W參數，因為numerical_gradient(f, x)在函數裡執行f(x)，所以要定義f(W)才有整合性\n",
    "f = lambda w: net.loss(x, t)\n",
    "\n",
    "graW = numerical_gradient(f, net.W)\n",
    "print('梯度: ',graW) # 得到的結果最左上角的代表，改變那個權重會影響整個function的變化量\n",
    "# 之後只要以梯度下降法更新梯度即可找到最佳權重!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.學習演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step1(小批次) ==> 從訓練資料中隨機挑選部分資料稱做小批次，此步驟以減少小批次的損失函數為目標\n",
    "# step2(計算梯度) ==> 為了減少小批次的損失函數，計算各權重參數的梯度，梯度表示損失函數減少最多的方向\n",
    "# step3(更新參數) ==> 權重參數往梯度方向更新\n",
    "# 重複上述步驟\n",
    "# 這裡使用的是 \"準確率梯度下降法\" (stochastic gradient descent)，準確率代表準確隨機挑選，縮寫SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 雙層神經網路的示範，一個隱藏層\n",
    "class TwoLayerNet:\n",
    "    # 權重初始化\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        # W1的shape是input_size*hidden_size，才可以和input矩陣做相乘\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:輸入資料，t:訓練資料(正確標籤)\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:輸入資料，t:訓練資料(正確標籤)\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "#         dy = (y - t) / batch_num\n",
    "#         grads['W2'] = np.dot(z1.T, dy)\n",
    "#         grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "#         da1 = np.dot(dy, W2.T)\n",
    "#         dz1 = sigmoid_grad(a1) * da1\n",
    "#         grads['W1'] = np.dot(x.T, dz1)\n",
    "#         grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 載入資料集\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x_train = mnist.train.images\n",
    "t_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "t_test = mnist.test.labels\n",
    "# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient_2d(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_1d(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_1d(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.11234545454545454, 0.1135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ee25314d6089>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# 計算梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m#     grad = network.gradient(x_batch, t_batch) # 反向傳播法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4c0a9b862053>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-15bfd35f08b6>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mtmp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mfxh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# f(x+h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4c0a9b862053>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# x:輸入資料，t:訓練資料(正確標籤)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mloss_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4c0a9b862053>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# x:輸入資料，t:訓練資料(正確標籤)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4c0a9b862053>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  # 梯度法更新迭代的次數\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 小批次數量\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1epoch代表在學習中用完所有資料的次數，10000筆資料做100個小批次學習，此時100次 = 1epoch\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 計算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "#     grad = network.gradient(x_batch, t_batch) # 反向傳播法\n",
    "    \n",
    "    # 更新參數\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 儲存學習過程的損失函數值\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        # 每100次記錄一次訓練資料和測試資料的準確率\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 畫圖\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
